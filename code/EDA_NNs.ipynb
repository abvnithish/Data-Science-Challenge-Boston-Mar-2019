{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Including data \n",
    "\n",
    "new_x_train = pd.read_csv('train.csv')\n",
    "new_y_train = new_x_train.Dep_Var\n",
    "new_x_train=new_x_train.drop(['Unnamed: 0','Date','Dep_Var','Identifier','quarter'],axis=1)\n",
    "new_x_val = pd.read_csv('validation.csv')\n",
    "new_y_val = new_x_val.Dep_Var\n",
    "new_x_val=new_x_val.drop(['Unnamed: 0','Date','Dep_Var','Identifier','quarter'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Regression NN\n",
    "\n",
    "input_pats = torch.tensor(np.array(new_x_train),dtype=torch.float)\n",
    "output_pats = torch.tensor(np.array(new_y_train),dtype=torch.long)\n",
    "\n",
    "test_input_pats = torch.tensor(np.array(new_x_val),dtype=torch.float)\n",
    "test_output_pats = torch.tensor(np.array(new_y_val),dtype=torch.long)\n",
    "\n",
    "N = input_pats.shape[0]\n",
    "M = test_input_pats.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=106, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=20, bias=True)\n",
      "  (elu): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout1): Dropout(p=0.2)\n",
      "  (dropout2): Dropout(p=0.2)\n",
      "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (final_activation): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_pats.shape[1], 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 20)\n",
    "        self.elu = torch.nn.LeakyReLU()\n",
    "        self.dropout1 = torch.nn.Dropout(0.2)\n",
    "        self.dropout2 = torch.nn.Dropout(0.2)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm1d(64,affine=False)\n",
    "        self.batchnorm2 = torch.nn.BatchNorm1d(32,affine=False)\n",
    "        self.final_activation = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, input_pats.shape[1])\n",
    "        x = self.elu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.elu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.final_activation(self.fc3(x))\n",
    "        return x.view(-1,20)\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train(mynet,epoch_count,nepochs,batch_size=16):\n",
    "    for e in range(nepochs):\n",
    "        mynet.train()\n",
    "        error_epoch = 0\n",
    "        permutation = torch.randperm(N)\n",
    "        for i in range(0,N,batch_size):\n",
    "            mynet.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            \n",
    "            batch_x, batch_y = input_pats[indices], output_pats[indices]\n",
    "            output = mynet(batch_x) \n",
    "            target = batch_y\n",
    "            loss = criterion(output, target) \n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "            error_epoch += loss.item()\n",
    "        with torch.no_grad():\n",
    "            mynet.eval()\n",
    "            val_loss = 0\n",
    "            for t in range(M):\n",
    "                output = mynet(test_input_pats[t]) \n",
    "                target = test_output_pats[t]\n",
    "                val_loss += criterion(output, target.view(-1)) \n",
    "            val_loss /= float(M)\n",
    "            scheduler.step(val_loss)\n",
    "        error_epoch = error_epoch / float(N)        \n",
    "        print('epoch ' + str(e) +' loss ' + str(round(error_epoch,3)) + ' val loss ' + str(round(val_loss.item(),3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.186 val loss 2.976\n",
      "epoch 1 loss 0.186 val loss 2.976\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "mynet = Net()\n",
    "optimizer = torch.optim.Adam(mynet.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',patience=1)\n",
    "nepochs = 2\n",
    "batch_train(mynet,scheduler,nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({12: 2702, 3: 2006, 0: 1490, 10: 1439, 8: 1257, 1: 1244, 18: 958, 13: 402, 16: 400, 11: 377, 2: 59, 14: 35, 7: 33, 4: 6, 19: 5, 9: 5, 5: 2, 6: 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.03393905294663558, pvalue=0.00015482430183892505)"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = test_input_pats\n",
    "T = len(test_set)\n",
    "with torch.no_grad():\n",
    "    pred_val = mynet(test_set)\n",
    "final_pred = pred_val.numpy()\n",
    "final_pred = [np.argmax(f) for f in final_pred]\n",
    "from collections import Counter\n",
    "print(Counter(final_pred))\n",
    "stats.spearmanr(final_pred,new_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Classification NN\n",
    "\n",
    "\n",
    "input_pats = torch.tensor(np.array(new_x_train),dtype=torch.float)\n",
    "output_pats = torch.tensor(np.array(new_y_train),dtype=torch.float)\n",
    "\n",
    "test_input_pats = torch.tensor(np.array(new_x_val),dtype=torch.float)\n",
    "test_output_pats = torch.tensor(np.array(new_y_val),dtype=torch.float)\n",
    "\n",
    "N = input_pats.shape[0]\n",
    "M = test_input_pats.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=106, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (elu): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout1): Dropout(p=0.2)\n",
      "  (dropout2): Dropout(p=0.2)\n",
      "  (dropout3): Dropout(p=0.2)\n",
      "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (batchnorm3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (final_activation): Softplus(beta=1, threshold=20)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_pats.shape[1], 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "        self.elu = torch.nn.LeakyReLU()\n",
    "        self.dropout1 = torch.nn.Dropout(0.2)\n",
    "        self.dropout2 = torch.nn.Dropout(0.2)\n",
    "        self.dropout3 = torch.nn.Dropout(0.2)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm1d(64,affine=False)\n",
    "        self.batchnorm2 = torch.nn.BatchNorm1d(32,affine=False)\n",
    "        self.batchnorm3 = torch.nn.BatchNorm1d(16)\n",
    "        self.final_activation = torch.nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, input_pats.shape[1])\n",
    "        x = self.elu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.elu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.final_activation(self.fc4(x))\n",
    "        return x.view(-1)\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "def batch_train(mynet,epoch_count,nepochs,batch_size=16):\n",
    "    for e in range(nepochs):\n",
    "        mynet.train()\n",
    "        error_epoch = 0\n",
    "        permutation = torch.randperm(N)\n",
    "        for i in range(0,N,batch_size):\n",
    "            mynet.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x, batch_y = input_pats[indices], output_pats[indices]\n",
    "            output = mynet(batch_x) \n",
    "            target = batch_y\n",
    "            loss = criterion(output, target) \n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "            error_epoch += loss.item()\n",
    "        with torch.no_grad():\n",
    "            mynet.eval()\n",
    "            val_loss = 0\n",
    "            for t in range(M):\n",
    "                output = mynet(test_input_pats[t]) \n",
    "                target = test_output_pats[t]\n",
    "                val_loss += criterion(output, target.view(-1)) \n",
    "            val_loss /= float(M)\n",
    "            scheduler.step(val_loss)\n",
    "        error_epoch = error_epoch / float(N)        \n",
    "        print('epoch ' + str(e) +' loss ' + str(round(error_epoch,3)) + ' val loss ' + str(round(val_loss.item(),3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.316 val loss 4.954\n",
      "epoch 1 loss 0.308 val loss 4.974\n",
      "epoch 2 loss 0.305 val loss 4.976\n",
      "epoch 3 loss 0.302 val loss 5.007\n",
      "epoch 4 loss 0.301 val loss 5.017\n",
      "epoch 5 loss 0.3 val loss 5.005\n",
      "epoch 6 loss 0.3 val loss 5.01\n",
      "epoch 7 loss 0.3 val loss 4.999\n",
      "epoch 8 loss 0.3 val loss 4.99\n",
      "epoch 9 loss 0.299 val loss 5.024\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "criterion = nn.L1Loss()\n",
    "mynet = Net()\n",
    "optimizer = torch.optim.Adam(mynet.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',patience=1)\n",
    "nepochs = 10\n",
    "batch_train(mynet,scheduler,nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 622, 2: 621, 3: 621, 12: 621, 6: 621, 15: 621, 9: 621, 4: 621, 17: 621, 18: 621, 13: 621, 7: 621, 16: 621, 19: 621, 10: 621, 14: 621, 1: 621, 11: 621, 8: 621, 5: 621})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=-0.009962479078470267, pvalue=0.2669001359592835)"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = test_input_pats\n",
    "T = len(test_set)\n",
    "with torch.no_grad():\n",
    "    pred_val = mynet(test_set)\n",
    "ranked_pred=pred_val.numpy().argsort()\n",
    "bins = [i for i in np.arange(0,T,T/20)]\n",
    "bins.append(T)\n",
    "\n",
    "final_pred = []\n",
    "for r in ranked_pred:\n",
    "    for i in range(20):\n",
    "        if bins[i+1]>r:\n",
    "            final_pred.append(i)\n",
    "            break\n",
    "from collections import Counter\n",
    "print(Counter(final_pred))\n",
    "from scipy import stats\n",
    "stats.spearmanr(final_pred,new_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
